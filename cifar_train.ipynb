{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from vit_pytorch import ViT\n",
    "from vit_pytorch.mobile_vit import MobileViT\n",
    "from vit_pytorch.crossformer import CrossFormer\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard writer\n",
    "log_dir = \"./logs/vit_cifar10\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "image_size = 32  # CIFAR-10 image size\n",
    "patch_size = 4   # Patch size for ViT\n",
    "num_classes = 10\n",
    "dim = 128        # Embedding dimension\n",
    "depth = 6        # Number of transformer layers\n",
    "heads = 8        # Number of attention heads\n",
    "mlp_dim = 256    # Dimension of MLP layer\n",
    "dropout = 0.1\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 5000 | Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "# Choose only the first 5000 examples for training and testing\n",
    "train_loader.dataset.data = train_loader.dataset.data[:5000]\n",
    "train_loader.dataset.targets = train_loader.dataset.targets[:5000]\n",
    "test_loader.dataset.data = test_loader.dataset.data[:1000]\n",
    "test_loader.dataset.targets = test_loader.dataset.targets[:1000]\n",
    "\n",
    "print(f\"Number of training examples: {len(train_dataset)} | Number of testing examples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossFormer(\n",
      "  (layers): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): CrossEmbedLayer(\n",
      "        (convs): ModuleList(\n",
      "          (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(4, 4))\n",
      "          (1): Conv2d(3, 8, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))\n",
      "          (2): Conv2d(3, 4, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))\n",
      "          (3): Conv2d(3, 4, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))\n",
      "        )\n",
      "      )\n",
      "      (1): Transformer(\n",
      "        (layers): ModuleList(\n",
      "          (0): ModuleList(\n",
      "            (0): Attention(\n",
      "              (norm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (to_qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (to_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (dpb): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=8, bias=True)\n",
      "                (1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (2): ReLU()\n",
      "                (3): Linear(in_features=8, out_features=8, bias=True)\n",
      "                (4): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (5): ReLU()\n",
      "                (6): Linear(in_features=8, out_features=8, bias=True)\n",
      "                (7): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (8): ReLU()\n",
      "                (9): Linear(in_features=8, out_features=1, bias=True)\n",
      "                (10): Rearrange('... () -> ...')\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): LayerNorm()\n",
      "              (1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (2): GELU(approximate='none')\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "              (4): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (2): Attention(\n",
      "              (norm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (to_qkv): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (to_out): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (dpb): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=8, bias=True)\n",
      "                (1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (2): ReLU()\n",
      "                (3): Linear(in_features=8, out_features=8, bias=True)\n",
      "                (4): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (5): ReLU()\n",
      "                (6): Linear(in_features=8, out_features=8, bias=True)\n",
      "                (7): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "                (8): ReLU()\n",
      "                (9): Linear(in_features=8, out_features=1, bias=True)\n",
      "                (10): Rearrange('... () -> ...')\n",
      "              )\n",
      "            )\n",
      "            (3): Sequential(\n",
      "              (0): LayerNorm()\n",
      "              (1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (2): GELU(approximate='none')\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "              (4): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): CrossEmbedLayer(\n",
      "        (convs): ModuleList(\n",
      "          (0): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "          (1): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (1): Transformer(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x ModuleList(\n",
      "            (0): Attention(\n",
      "              (norm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (to_qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (to_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (dpb): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "                (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (2): ReLU()\n",
      "                (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "                (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (5): ReLU()\n",
      "                (6): Linear(in_features=16, out_features=16, bias=True)\n",
      "                (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (8): ReLU()\n",
      "                (9): Linear(in_features=16, out_features=1, bias=True)\n",
      "                (10): Rearrange('... () -> ...')\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): LayerNorm()\n",
      "              (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (2): GELU(approximate='none')\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "              (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (2): Attention(\n",
      "              (norm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (to_qkv): Conv2d(64, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (to_out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (dpb): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "                (1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (2): ReLU()\n",
      "                (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "                (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (5): ReLU()\n",
      "                (6): Linear(in_features=16, out_features=16, bias=True)\n",
      "                (7): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "                (8): ReLU()\n",
      "                (9): Linear(in_features=16, out_features=1, bias=True)\n",
      "                (10): Rearrange('... () -> ...')\n",
      "              )\n",
      "            )\n",
      "            (3): Sequential(\n",
      "              (0): LayerNorm()\n",
      "              (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (2): GELU(approximate='none')\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "              (4): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): CrossEmbedLayer(\n",
      "        (convs): ModuleList(\n",
      "          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (1): Transformer(\n",
      "        (layers): ModuleList(\n",
      "          (0-3): 4 x ModuleList(\n",
      "            (0): Attention(\n",
      "              (norm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (to_qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (to_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (dpb): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "                (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (2): ReLU()\n",
      "                (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "                (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (5): ReLU()\n",
      "                (6): Linear(in_features=32, out_features=32, bias=True)\n",
      "                (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (8): ReLU()\n",
      "                (9): Linear(in_features=32, out_features=1, bias=True)\n",
      "                (10): Rearrange('... () -> ...')\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): LayerNorm()\n",
      "              (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (2): GELU(approximate='none')\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "              (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (2): Attention(\n",
      "              (norm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (to_qkv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (to_out): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (dpb): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "                (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (2): ReLU()\n",
      "                (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "                (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (5): ReLU()\n",
      "                (6): Linear(in_features=32, out_features=32, bias=True)\n",
      "                (7): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                (8): ReLU()\n",
      "                (9): Linear(in_features=32, out_features=1, bias=True)\n",
      "                (10): Rearrange('... () -> ...')\n",
      "              )\n",
      "            )\n",
      "            (3): Sequential(\n",
      "              (0): LayerNorm()\n",
      "              (1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (2): GELU(approximate='none')\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "              (4): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): CrossEmbedLayer(\n",
      "        (convs): ModuleList(\n",
      "          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (1): Transformer(\n",
      "        (layers): ModuleList(\n",
      "          (0): ModuleList(\n",
      "            (0): Attention(\n",
      "              (norm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (to_qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (to_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (dpb): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "                (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (2): ReLU()\n",
      "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (5): ReLU()\n",
      "                (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (8): ReLU()\n",
      "                (9): Linear(in_features=64, out_features=1, bias=True)\n",
      "                (10): Rearrange('... () -> ...')\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): LayerNorm()\n",
      "              (1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (2): GELU(approximate='none')\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "              (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (2): Attention(\n",
      "              (norm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (to_qkv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (to_out): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (dpb): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "                (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (2): ReLU()\n",
      "                (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (5): ReLU()\n",
      "                (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (7): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "                (8): ReLU()\n",
      "                (9): Linear(in_features=64, out_features=1, bias=True)\n",
      "                (10): Rearrange('... () -> ...')\n",
      "              )\n",
      "            )\n",
      "            (3): Sequential(\n",
      "              (0): LayerNorm()\n",
      "              (1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (2): GELU(approximate='none')\n",
      "              (3): Dropout(p=0.0, inplace=False)\n",
      "              (4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (to_logits): Sequential(\n",
      "    (0): Reduce('b c h w -> b c', 'mean')\n",
      "    (1): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load ViT model\n",
    "\n",
    "# Define the Vision Transformer model\n",
    "# model = ViT(\n",
    "#     image_size=image_size,\n",
    "#     patch_size=patch_size,\n",
    "#     num_classes=num_classes,\n",
    "#     dim=dim,\n",
    "#     depth=depth,\n",
    "#     heads=heads,\n",
    "#     mlp_dim=mlp_dim,\n",
    "#     dropout=dropout,\n",
    "#     emb_dropout=dropout\n",
    "# ).to(device)\n",
    "\n",
    "model = CrossFormer(\n",
    "    num_classes=10,                 # CIFAR-10 has 10 classes\n",
    "    dim=(32, 64, 128, 256),         # Reduced dimensions for smaller dataset\n",
    "    depth=(1, 2, 4, 1),             # Shallower model\n",
    "    global_window_size=(4, 2, 1, 1), # Adjusted for CIFAR-10\n",
    "    local_window_size=1,            # Default local window size\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Model Summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch [1/50], Loss: 1.9820, Test Accuracy: 31.50%\n",
      "Epoch [2/50], Loss: 1.7032, Test Accuracy: 37.90%\n",
      "Epoch [3/50], Loss: 1.5820, Test Accuracy: 40.00%\n",
      "Epoch [4/50], Loss: 1.4552, Test Accuracy: 41.20%\n",
      "Epoch [5/50], Loss: 1.3348, Test Accuracy: 41.30%\n",
      "Epoch [6/50], Loss: 1.2479, Test Accuracy: 41.40%\n",
      "Epoch [7/50], Loss: 1.1090, Test Accuracy: 42.40%\n",
      "Epoch [8/50], Loss: 1.0702, Test Accuracy: 41.00%\n",
      "Epoch [9/50], Loss: 0.9913, Test Accuracy: 43.80%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m     36\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 38\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Code/vit-pytorch/vit_pytorch/crossformer.py:265\u001b[0m, in \u001b[0;36mCrossFormer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cel, transformer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    264\u001b[0m     x \u001b[38;5;241m=\u001b[39m cel(x)\n\u001b[0;32m--> 265\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_logits(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Code/vit-pytorch/vit_pytorch/crossformer.py:201\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    199\u001b[0m     x \u001b[38;5;241m=\u001b[39m short_attn(x) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    200\u001b[0m     x \u001b[38;5;241m=\u001b[39m short_ff(x) \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m--> 201\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlong_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    202\u001b[0m     x \u001b[38;5;241m=\u001b[39m long_ff(x) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Code/vit-pytorch/vit_pytorch/crossformer.py:124\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;241m*\u001b[39m_, height, width, heads, wsz, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, x\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# prenorm\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# rearrange for short or long distance attention\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshort\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Code/vit-pytorch/vit_pytorch/crossformer.py:66\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     65\u001b[0m     var \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvar(x, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, unbiased \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, keepdim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 66\u001b[0m     mean \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (x \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m (var \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    # Log training loss to TensorBoard\n",
    "    writer.add_scalar(\"Loss/Train\", train_loss, epoch + 1)\n",
    "\n",
    "    # Testing loop\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    test_accuracy_history.append(accuracy)\n",
    "\n",
    "    # Log test accuracy to TensorBoard\n",
    "    writer.add_scalar(\"Accuracy/Test\", accuracy, epoch + 1)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on Test Data...\")\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "class_names = test_dataset.classes\n",
    "\n",
    "# Plot confusion matrix as an image for TensorBoard\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "\n",
    "# Save confusion matrix plot to TensorBoard\n",
    "plt.tight_layout()\n",
    "writer.add_figure(\"Confusion erfgerfgweryMatrix\", plt.gcf())\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test images with predictions and ground truth\n",
    "import random\n",
    "\n",
    "def visualize_test_images(model, test_loader, class_names, num_images=16):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    images, labels = next(iter(test_loader))  # Get a batch of test images\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Select random indices\n",
    "    indices = random.sample(range(len(images)), num_images)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i, idx in enumerate(indices):\n",
    "        image = images[idx].cpu().numpy().transpose(1, 2, 0)  # Convert to HWC format\n",
    "        image = (image * 0.5) + 0.5  # Unnormalize\n",
    "        label = labels[idx].item()\n",
    "        prediction = predicted[idx].item()\n",
    "\n",
    "        plt.subplot(4, 4, i + 1)  # Arrange in a 4x4 grid\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"True: {class_names[label]}\\nPred: {class_names[prediction]}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Optionally log the visualization to TensorBoard\n",
    "    writer.add_figure(\"Test Images\", plt.gcf())\n",
    "\n",
    "# Call the function to visualize\n",
    "visualize_test_images(model, test_loader, class_names, num_images=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(model, data_loader, num_layers=6, patch_size=4):\n",
    "    \"\"\"\n",
    "    Visualizes attention maps for a random image after each transformer layer.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Vision Transformer model.\n",
    "        data_loader: DataLoader for test data.\n",
    "        num_layers: Number of transformer layers in the model.\n",
    "        patch_size: Patch size used in ViT.\n",
    "    \"\"\"\n",
    "    # Hook to store attention maps\n",
    "    attention_maps = []\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        # The attention weights are typically in `module.attention`\n",
    "        attention_maps.append(module.attention_weights)\n",
    "\n",
    "    # Register hooks for each transformer block\n",
    "    hooks = []\n",
    "    for i in range(num_layers):\n",
    "        hooks.append(\n",
    "            model.transformer.layers[i].register_forward_hook(hook)\n",
    "        )\n",
    "\n",
    "    # Select a random image from the test loader\n",
    "    images, labels = next(iter(data_loader))\n",
    "    random_idx = random.randint(0, images.size(0) - 1)\n",
    "    image = images[random_idx:random_idx + 1].to(device)\n",
    "    label = labels[random_idx].item()\n",
    "\n",
    "    # Pass the image through the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _ = model(image)\n",
    "\n",
    "    # Plot the original image\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, num_layers + 1, 1)\n",
    "    plt.imshow((image[0].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5).clip(0, 1))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Original Image\")\n",
    "\n",
    "    # Plot attention maps\n",
    "    for i, attention in enumerate(attention_maps):\n",
    "        attention = attention[0].mean(dim=0).cpu().numpy()  # Take mean attention over heads\n",
    "        num_patches = int(attention.shape[0] ** 0.5)\n",
    "\n",
    "        # Reshape attention to match patches\n",
    "        attention = attention.reshape(num_patches, num_patches)\n",
    "        attention = torch.nn.functional.interpolate(\n",
    "            torch.tensor(attention).unsqueeze(0).unsqueeze(0),\n",
    "            size=(image.size(2), image.size(3)),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        ).squeeze().cpu().numpy()\n",
    "\n",
    "        plt.subplot(1, num_layers + 1, i + 2)\n",
    "        plt.imshow(attention, cmap=\"viridis\")\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Layer {i + 1}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "# Call the visualization function\n",
    "visualize_attention(model, test_loader, num_layers=depth, patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_maps(model, images):\n",
    "    \"\"\"\n",
    "    Captures attention maps from the Vision Transformer model.\n",
    "    Args:\n",
    "        model: Vision Transformer model.\n",
    "        images: Input batch of images.\n",
    "    Returns:\n",
    "        List of attention maps for each transformer layer.\n",
    "    \"\"\"\n",
    "    attention_maps = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        attention_maps.append(output)\n",
    "\n",
    "    # Register hooks for each transformer block\n",
    "    hooks = []\n",
    "    for transformer_block in model.transformer.blocks:\n",
    "        hooks.append(transformer_block.attn.attn_drop.register_forward_hook(hook_fn))\n",
    "\n",
    "    # Perform a forward pass to capture attention maps\n",
    "    with torch.no_grad():\n",
    "        _ = model(images)\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(images, attention_maps, patch_size, image_size, num_heads=8):\n",
    "    \"\"\"\n",
    "    Visualizes the attention maps for a batch of images.\n",
    "    Args:\n",
    "        images: Batch of input images (torch.Tensor).\n",
    "        attention_maps: List of attention maps from ViT.\n",
    "        patch_size: Patch size used in the ViT model.\n",
    "        image_size: Image size of the input images.\n",
    "        num_heads: Number of attention heads.\n",
    "    \"\"\"\n",
    "    batch_size = images.size(0)\n",
    "    num_layers = len(attention_maps)\n",
    "\n",
    "    # Rescale images to [0, 1] for visualization\n",
    "    images = images.permute(0, 2, 3, 1).cpu().numpy() * 0.5 + 0.5\n",
    "\n",
    "    for idx in range(batch_size):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.suptitle(f\"Image {idx + 1}: Attention Maps\", fontsize=16)\n",
    "\n",
    "        # Plot the original image\n",
    "        plt.subplot(2, num_layers + 1, 1)\n",
    "        plt.imshow(images[idx])\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Original Image\")\n",
    "\n",
    "        # Loop through each layer\n",
    "        for layer_idx, layer_attentions in enumerate(attention_maps):\n",
    "            # Extract the attention map for this image and layer\n",
    "            attention = layer_attentions[idx]  # Shape: (num_heads, num_patches, num_patches)\n",
    "            attention = attention.mean(dim=0).reshape(patch_size, patch_size)  # Average over heads\n",
    "\n",
    "            # Resize attention map to match the image size\n",
    "            attention_resized = cv2.resize(attention.cpu().numpy(), (image_size, image_size))\n",
    "            attention_resized = (attention_resized - attention_resized.min()) / (attention_resized.max() - attention_resized.min())\n",
    "\n",
    "            # Overlay attention on the original image\n",
    "            overlay = images[idx].copy()\n",
    "            heatmap = cv2.applyColorMap((attention_resized * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "            heatmap = heatmap[..., ::-1] / 255.0\n",
    "            overlay = cv2.addWeighted(overlay, 0.5, heatmap, 0.5, 0)\n",
    "\n",
    "            plt.subplot(2, num_layers + 1, layer_idx + 2)\n",
    "            plt.imshow(overlay)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"Layer {layer_idx + 1}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a batch of test images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Get a batch of test images\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "test_images = test_images.to(device)\n",
    "\n",
    "# Capture attention maps\n",
    "attention_maps = get_attention_maps(model, test_images)\n",
    "\n",
    "# Visualize the attention maps\n",
    "visualize_attention(test_images, attention_maps, patch_size=patch_size, image_size=image_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
