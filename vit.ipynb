{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lelouch/miniconda3/envs/vit/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(urlopen(\n",
    "    'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('vit_mediumd_patch16_reg4_gap_256.sbb_in12k_ft_in1k', pretrained=True)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(3, 512, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (patch_drop): Identity()\n",
      "  (norm_pre): Identity()\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (12): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (13): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (14): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (15): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (16): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (17): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (18): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "    (19): Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (q_norm): Identity()\n",
      "        (k_norm): Identity()\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls1): LayerScale()\n",
      "      (drop_path1): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.0, inplace=False)\n",
      "        (norm): Identity()\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop2): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ls2): LayerScale()\n",
      "      (drop_path2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (norm): Identity()\n",
      "  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "  (head_drop): Dropout(p=0.0, inplace=False)\n",
      "  (head): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "output = model(transforms(img).unsqueeze(0))  # unsqueeze single image into batch of 1\n",
    "\n",
    "top5_probabilities, top5_class_indices = torch.topk(output.softmax(dim=1) * 100, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[52.7900, 10.1153,  7.2810,  4.4503,  1.5161]],\n",
      "       grad_fn=<TopkBackward0>)\n",
      "tensor([[967, 504, 968, 415, 960]])\n",
      "tensor([[-1.0360e-01, -7.5831e-01, -1.3608e-01, -3.9874e-02, -5.7988e-01,\n",
      "          4.6911e-01,  9.7656e-03, -7.1957e-01, -2.8360e-01, -7.7765e-02,\n",
      "          5.2384e-01, -2.1930e-01,  3.1100e-01, -6.9803e-02, -1.5805e-01,\n",
      "          9.5728e-02,  1.6710e-01, -2.4809e-01, -5.6220e-01, -1.0428e-01,\n",
      "         -5.1519e-01,  2.6381e-01,  1.0474e+00,  7.2704e-01,  1.1095e-01,\n",
      "         -2.9215e-01, -7.7121e-01, -7.2072e-01, -1.5735e+00, -1.3539e+00,\n",
      "         -2.3654e-01,  4.7543e-01,  1.4705e-01,  7.0121e-01, -3.0874e-01,\n",
      "         -3.6605e-01, -2.5866e-01, -9.5356e-02,  3.4971e-01,  4.6263e-01,\n",
      "         -9.9917e-02,  1.0015e-03,  9.2637e-01,  4.9731e-01, -6.2342e-01,\n",
      "          1.4665e-01, -7.3544e-02, -1.5473e-01, -6.1598e-01,  4.2367e-01,\n",
      "         -6.5245e-01, -2.9789e-01, -8.3461e-01,  3.1077e-02,  1.4136e-01,\n",
      "          5.1430e-01, -7.8582e-01, -1.7560e-01, -3.6081e-01, -9.3154e-02,\n",
      "         -6.4660e-03, -7.9723e-02,  2.1352e-02,  1.3639e-01, -1.1037e-02,\n",
      "          1.0635e-01,  4.9759e-01, -8.4077e-01,  6.9127e-01, -3.8265e-01,\n",
      "         -1.3206e-01, -9.3933e-01,  9.3323e-02, -8.7523e-01,  5.5778e-02,\n",
      "         -6.5985e-01, -8.5867e-01, -7.8348e-01, -3.8344e-01, -5.4831e-01,\n",
      "          7.2577e-01, -2.3698e-01,  9.3904e-01, -4.6044e-02, -4.4137e-01,\n",
      "         -3.7562e-01,  1.0718e+00, -2.5830e-01, -4.3147e-01, -4.5789e-01,\n",
      "         -2.7846e-01, -2.5893e-01, -2.3627e-01, -4.8842e-02, -4.2590e-01,\n",
      "         -5.2603e-01, -3.0736e-01, -4.7840e-01, -4.0995e-02, -6.1459e-01,\n",
      "          5.6195e-01, -7.2212e-01, -2.8429e-01, -2.1220e-01,  7.7296e-02,\n",
      "          4.8891e-01,  1.8970e-01,  1.7667e-01,  4.0672e-01,  7.6839e-01,\n",
      "         -7.2645e-01, -2.0326e-01,  8.8835e-01,  9.0727e-01,  3.9454e-01,\n",
      "         -5.6726e-01, -3.4603e-01, -6.6745e-02, -1.2468e-01,  6.7052e-01,\n",
      "         -9.2603e-01, -1.6931e-01, -4.6779e-01, -2.5751e-01, -8.5268e-01,\n",
      "          4.1225e-01, -7.1285e-01,  3.0691e-01,  3.0897e-01,  8.3111e-02,\n",
      "         -6.2604e-01,  4.2447e-01,  3.7822e-01,  2.0266e-01,  1.3929e-01,\n",
      "          4.7318e-01, -2.3245e-01, -6.6912e-01,  3.4344e-01, -5.7371e-01,\n",
      "         -4.4197e-01, -5.9051e-01, -1.3549e-01,  1.9844e-01,  8.0059e-01,\n",
      "         -1.3746e+00, -2.2918e-01, -6.6656e-02,  1.0125e+00, -6.4709e-01,\n",
      "          4.5184e-01, -6.4738e-01,  2.2567e-01,  1.1152e-01,  7.4258e-01,\n",
      "         -3.4044e-01,  5.0783e-01,  7.8255e-02,  2.9946e-01, -3.0472e-01,\n",
      "         -3.3115e-01, -4.6963e-01,  2.9151e-01,  3.6480e-01,  2.3401e-01,\n",
      "          6.9588e-01,  3.9591e-02, -1.5197e-01,  2.4891e-01, -2.4245e-01,\n",
      "          4.0009e-01,  1.7781e-01,  3.3627e-01,  4.9755e-02, -3.3472e-01,\n",
      "         -5.5975e-01,  3.0299e-02, -6.8562e-01,  1.4214e-01, -5.7256e-01,\n",
      "          3.1923e-01, -2.7230e-01,  6.4557e-02, -1.0453e-01, -2.9579e-01,\n",
      "          5.1910e-01, -7.9263e-02,  5.8082e-01,  4.0953e-01,  8.8363e-01,\n",
      "         -4.8904e-02,  1.2262e+00,  2.9731e-01,  4.7471e-01, -8.4819e-02,\n",
      "          1.8694e-01, -4.7986e-01, -2.1880e-01, -8.2038e-02,  6.2535e-01,\n",
      "         -1.9724e-01,  8.7751e-01, -2.4795e-01,  3.8286e-01, -4.1509e-01,\n",
      "          2.2833e-01,  3.5765e-01, -2.9114e-01,  4.2905e-01, -4.0061e-01,\n",
      "          2.7070e-02, -6.4584e-01, -5.1456e-01, -1.5964e+00, -9.7081e-02,\n",
      "         -4.7235e-01, -5.8859e-01, -3.1873e-01, -9.9193e-01, -4.2459e-02,\n",
      "         -6.1121e-01, -3.5778e-03,  2.3322e-01, -1.0392e-01,  5.8799e-02,\n",
      "          8.1464e-01, -3.7493e-01,  3.9428e-01,  1.3023e-01, -6.3358e-02,\n",
      "          3.6576e-01,  5.9539e-01,  3.0439e-02, -1.0452e-01, -2.7525e-01,\n",
      "          1.3234e-01,  1.6362e-01, -3.8575e-01, -6.3969e-01, -1.5114e-02,\n",
      "         -4.0360e-01, -1.9570e-01, -5.0420e-01,  1.6082e-01, -2.0973e-01,\n",
      "         -1.4211e-01,  9.6660e-02, -8.1506e-02, -7.2809e-01, -3.6311e-01,\n",
      "         -3.1642e-01, -4.4895e-01,  7.6211e-02, -3.4673e-01, -7.3422e-01,\n",
      "         -4.0324e-01,  2.9837e-02,  5.5483e-01, -1.5927e-01, -2.9099e-01,\n",
      "         -5.1544e-01,  1.2112e-01, -1.3119e+00, -6.4280e-01, -1.4033e-01,\n",
      "         -2.5345e-01, -4.1419e-01,  1.6337e-01, -5.0802e-01, -2.1624e-01,\n",
      "         -5.0596e-01,  2.8544e-01,  6.1432e-02,  4.2636e-01, -6.8651e-01,\n",
      "         -2.0609e-01,  9.1837e-02,  2.5189e-01, -2.6544e-01, -4.1964e-01,\n",
      "          1.1411e-01, -5.6279e-01, -4.7471e-01, -1.6845e-01,  1.2191e-01,\n",
      "         -4.5904e-01,  7.3969e-01, -5.0549e-01, -2.0975e-01, -4.5755e-01,\n",
      "          5.7079e-01,  4.0388e-01,  1.7087e-01, -1.3965e-01, -6.0544e-02,\n",
      "         -7.8642e-02,  1.1792e-01, -1.2293e-01, -7.5029e-01, -3.0689e-01,\n",
      "         -9.8213e-01,  4.8859e-01, -3.0632e-01, -7.7586e-01, -1.4086e-01,\n",
      "         -4.2090e-01, -7.7774e-01, -2.9175e-01,  2.9970e-01, -1.0998e+00,\n",
      "          7.6236e-02,  6.8613e-02, -3.0672e-01, -1.9377e-01, -6.8617e-01,\n",
      "          3.5112e-01,  5.8692e-02,  8.1993e-02, -6.5423e-01,  2.7134e-01,\n",
      "          6.7333e-01, -6.6876e-01, -5.0923e-01, -5.0628e-01,  2.4953e-01,\n",
      "          3.7490e-02, -4.8438e-01, -3.0516e-02,  3.5423e-01, -1.6027e-01,\n",
      "         -5.9090e-01, -5.8037e-01,  2.9245e-01, -7.9121e-01,  2.6062e-01,\n",
      "         -2.6149e-01, -6.9257e-02, -3.5878e-01,  3.2591e-01, -1.6915e-01,\n",
      "         -2.8268e-01, -1.3980e-01, -4.7992e-01, -4.6177e-01, -1.7446e-01,\n",
      "          4.9392e-01, -4.0741e-01, -2.6951e-01,  9.1168e-02, -6.0807e-01,\n",
      "         -7.4366e-01, -2.0972e-01, -1.0342e-01, -2.0265e-01,  6.9005e-02,\n",
      "         -3.6516e-01,  1.4480e-01,  1.8336e-01, -1.1630e-01,  3.4087e-01,\n",
      "          3.7697e-01, -3.0412e-01,  6.9566e-01, -1.4442e-02,  2.8761e-01,\n",
      "          9.7625e-01, -5.9042e-01, -6.1510e-01, -5.5800e-01, -4.4619e-01,\n",
      "         -8.2618e-01, -2.5808e-01,  1.4545e-02, -1.8468e-01, -6.2394e-01,\n",
      "         -5.3616e-01, -3.8804e-01, -3.4287e-01, -1.7636e-01,  2.7240e-01,\n",
      "         -5.0174e-01, -2.1460e-01, -4.0325e-01, -1.2293e-01,  2.1798e-01,\n",
      "          1.4700e-01, -3.1350e-01, -3.7606e-01, -3.7470e-01,  6.0432e-01,\n",
      "         -1.7938e-01, -5.5741e-01,  2.5502e-01,  2.2492e-01, -6.3859e-01,\n",
      "          2.3043e-01,  2.5841e-01, -2.1362e-01, -9.3654e-01, -4.7794e-02,\n",
      "         -8.5333e-01, -6.9552e-01, -1.2296e+00, -5.4160e-01, -1.0017e+00,\n",
      "          4.0754e-01,  2.5154e-01, -1.7550e+00, -1.5142e-01, -2.5293e-01,\n",
      "         -1.1993e+00,  1.8527e-01,  3.1834e-02,  6.5442e-01,  2.6872e-01,\n",
      "          5.3877e+00, -2.7807e-01,  1.3356e-01,  8.0753e-02, -1.9022e+00,\n",
      "         -5.3516e-01, -8.1146e-01, -4.8162e-01,  2.5834e-03,  3.5301e-01,\n",
      "         -5.8496e-01, -2.4701e-01, -5.4668e-01, -1.6895e-01, -2.6805e-01,\n",
      "         -3.6246e-01,  2.7627e-01, -1.8006e-01,  1.4517e-01,  5.9656e-01,\n",
      "         -7.6172e-01, -1.1420e-01, -2.4191e-01, -9.1748e-01, -4.3545e-01,\n",
      "         -7.0236e-01, -1.9183e-01,  1.2344e+00,  1.6982e-01, -1.2669e-01,\n",
      "          2.3878e-01,  1.2368e-02, -3.6829e-01,  8.5653e-01, -5.9809e-02,\n",
      "          8.1525e-02, -8.7089e-01,  8.0950e-01, -3.5253e-01,  1.0400e+00,\n",
      "         -1.2528e+00, -9.7092e-01,  6.6034e-02,  5.6265e-01,  6.6817e-01,\n",
      "          3.3964e-01, -1.1256e-01, -4.4846e-01, -8.7798e-01, -5.5741e-01,\n",
      "         -5.3104e-01, -8.7750e-01, -9.8723e-01, -9.2072e-02,  1.8063e-01,\n",
      "         -4.8519e-01, -2.3390e-02, -6.2137e-01, -2.9105e-01, -4.3864e-03,\n",
      "         -3.5248e-01, -4.0358e-01,  3.8288e-01, -6.9665e-01,  7.1952e-02,\n",
      "         -5.9148e-01, -1.5102e+00, -1.0919e+00, -3.6150e-01,  1.8882e-01,\n",
      "         -7.2155e-01,  6.9146e-01,  1.6399e-01,  4.4511e-01, -4.3165e-01,\n",
      "         -6.6778e-01, -3.5958e-01, -2.9764e-01,  1.7584e-01, -4.0332e-02,\n",
      "         -4.0485e-01,  6.3405e-01, -5.1047e-01,  3.2237e-01, -1.0207e-01,\n",
      "         -4.9255e-01,  2.8143e-01, -2.4633e-01, -1.1581e+00,  6.2088e+00,\n",
      "          3.9818e+00,  2.6093e-01, -7.2816e-03, -2.1737e-01, -2.1177e-01,\n",
      "         -7.4382e-01,  2.9217e-02, -3.2526e-01,  1.1932e-01, -6.9948e-01,\n",
      "         -1.0074e+00, -6.0126e-01,  9.4232e-02, -3.0448e-02, -8.8214e-01,\n",
      "         -3.3082e-01, -1.3746e+00, -6.2672e-02, -3.7077e-01, -4.0288e-01,\n",
      "         -4.1070e-01,  1.3019e+00,  2.9809e-01, -4.9103e-01, -3.4682e-01,\n",
      "         -1.1729e+00, -1.2311e+00,  4.9438e-01,  4.3909e-02, -8.3927e-01,\n",
      "          8.7730e-02, -4.0282e-01, -6.5289e-02, -1.3484e-01,  5.6503e-01,\n",
      "          1.0781e-01, -3.6418e-01, -8.9072e-01,  1.7713e-01, -1.3003e-01,\n",
      "         -1.2439e+00, -5.4528e-01, -4.5229e-01, -3.6203e-01, -2.2632e-02,\n",
      "          2.5759e+00,  1.3131e+00, -2.8250e-01, -7.2154e-01, -6.3473e-01,\n",
      "         -6.1827e-01,  3.3362e-01,  1.1292e-01,  2.8862e-01, -1.3811e-01,\n",
      "         -6.7650e-01, -7.0508e-01,  8.3829e-01,  1.7176e-01,  5.2452e-01,\n",
      "         -6.1606e-01, -2.9452e-01, -5.4747e-01, -7.1087e-02, -5.9775e-01,\n",
      "         -5.4654e-01, -4.3385e-01,  6.6594e-02, -5.5845e-01,  1.9368e-02,\n",
      "         -2.2182e-01, -2.8282e-01, -3.2959e-01,  6.7026e-01,  1.5243e-01,\n",
      "         -4.6035e-01,  6.0719e-02,  6.6356e-01, -4.8142e-01,  2.6219e-01,\n",
      "         -3.4306e-01,  2.4976e-01,  3.7410e-01, -7.0645e-02, -2.9668e-02,\n",
      "         -4.3941e-01,  9.1259e-01, -6.4000e-01, -9.3542e-01,  2.0129e-02,\n",
      "         -8.7045e-02,  6.6726e-01, -1.3488e-01, -2.2782e-01, -2.2523e-01,\n",
      "         -9.3574e-02,  4.3895e-01, -5.2344e-01, -2.4439e-01, -9.6374e-01,\n",
      "          5.2926e-01, -7.2780e-01, -5.5318e-01, -6.0599e-01,  3.4663e-01,\n",
      "         -4.8394e-01,  6.8907e-01, -2.8873e-02, -4.0698e-01, -3.5814e-01,\n",
      "         -3.5596e-01,  2.5779e-01,  4.9490e-02,  4.4595e-01, -9.7023e-01,\n",
      "          3.3427e-01, -5.9380e-01,  2.5608e-01, -4.3846e-01,  4.4254e-01,\n",
      "         -2.6028e-01, -3.9735e-01, -2.7108e-01, -1.3279e-02,  2.8054e-01,\n",
      "         -1.9960e-01, -2.9513e-01,  7.0444e-01, -9.3613e-01, -2.2257e-01,\n",
      "         -8.4892e-01,  8.3310e-01,  5.1491e-01, -5.3740e-02, -3.5095e-01,\n",
      "          3.6148e-01, -4.6381e-01,  2.6168e-01,  6.1325e-01, -1.3724e-01,\n",
      "         -4.3954e-01, -6.5342e-01,  8.6406e-01, -5.0911e-01,  4.2503e-01,\n",
      "         -1.9418e-01, -1.2957e+00,  9.5206e-03, -6.2774e-01, -3.4767e-01,\n",
      "          7.7608e-01, -2.5672e-01, -2.1504e-01,  2.9437e-01,  6.6887e-02,\n",
      "         -7.6325e-01,  1.0148e-01, -3.4326e-01, -2.8723e-01,  2.5261e-01,\n",
      "         -7.1793e-02, -4.1654e-01, -2.9849e-01, -8.9695e-01, -9.5520e-01,\n",
      "          4.6481e-01, -3.4695e-01, -1.5791e-01,  7.0084e-02, -6.7110e-01,\n",
      "         -7.0622e-01,  1.3210e-01, -6.2598e-01, -4.0152e-01, -4.1295e-01,\n",
      "         -9.7069e-01,  1.4720e+00,  5.1759e-01,  5.3249e-01, -1.8799e-01,\n",
      "         -4.0436e-01,  5.1220e-02, -1.1934e+00, -1.8258e-01,  1.1760e-01,\n",
      "          1.8387e-01, -6.9139e-01, -3.3094e-01, -1.7080e-01, -2.2102e-01,\n",
      "          6.9329e-01,  1.0604e+00,  1.4164e-02,  4.1361e-01, -3.4216e-01,\n",
      "          7.4069e-01, -7.7080e-01, -1.8040e-01, -2.7029e-02, -6.3885e-01,\n",
      "         -5.1102e-01, -1.3779e-01, -3.6663e-01, -3.8936e-01, -3.2464e-01,\n",
      "         -4.6008e-01, -4.8832e-01, -1.0617e+00, -5.1827e-02,  6.1957e-02,\n",
      "         -7.4762e-01, -4.3552e-01,  8.2681e-02, -1.0140e-01,  1.5889e-01,\n",
      "         -9.6657e-01,  1.7427e-01, -5.3127e-01, -1.1699e+00, -3.3548e-01,\n",
      "          3.6561e-02, -1.3168e+00, -5.6115e-01, -9.7768e-01, -3.4940e-01,\n",
      "         -8.6795e-01, -7.6145e-02, -2.2892e-01, -9.0713e-01, -5.0651e-01,\n",
      "         -9.3011e-01, -9.4890e-01, -7.9771e-01,  6.7300e-01,  7.7871e-01,\n",
      "         -2.2489e-01,  2.2807e-02, -7.8493e-01, -6.0062e-01, -1.0294e+00,\n",
      "         -1.2873e+00, -2.4851e-01, -4.4475e-01,  4.7803e-01, -1.5437e-01,\n",
      "         -4.9090e-01,  3.5713e-01,  7.5231e-02,  2.3980e-01,  3.0682e-01,\n",
      "          3.3841e-01, -4.9557e-01, -4.0430e-01, -2.4103e-01,  1.7577e-01,\n",
      "         -6.3562e-01, -3.5575e-01,  2.8495e+00, -3.9500e-01,  2.7635e-01,\n",
      "          1.9355e-01, -1.0445e+00, -9.9527e-02, -8.5081e-01, -6.5373e-01,\n",
      "         -8.6078e-01,  2.2014e-01, -4.7815e-01,  1.3238e-01, -1.3318e+00,\n",
      "         -4.0417e-01, -5.9049e-01, -4.8710e-01, -8.6352e-01, -1.0778e+00,\n",
      "          6.6066e-02, -4.2436e-01,  3.3310e-01, -5.5710e-02, -8.7317e-01,\n",
      "         -8.1393e-01, -2.0842e-01,  4.3023e-01, -9.4706e-02, -1.2030e-01,\n",
      "         -7.7206e-01, -2.3039e-01, -4.7960e-02,  1.1471e-02, -7.0601e-01,\n",
      "         -5.7408e-01, -1.0362e-02,  1.2076e-01,  2.0246e-01, -5.7097e-01,\n",
      "         -1.8950e+00, -4.5020e-01,  7.3230e-01,  1.2406e-01, -1.3747e+00,\n",
      "         -8.7397e-01, -4.6423e-01,  4.0047e-01, -5.9392e-01,  1.8240e+00,\n",
      "         -8.5288e-02, -6.7836e-01,  5.0587e-01,  7.1677e-01, -2.4163e-02,\n",
      "         -1.6357e-01, -6.8522e-02,  4.1733e-01, -4.1433e-01, -2.0543e-02,\n",
      "         -2.7195e-01, -2.7504e-01, -5.9116e-01,  2.3018e-01, -2.6089e-01,\n",
      "         -1.2704e-01, -1.1549e+00, -3.4843e-01,  5.6710e-01,  1.1291e-01,\n",
      "         -7.5509e-01, -1.0118e-02, -2.4625e-01, -3.8755e-01, -2.8547e-01,\n",
      "         -2.7554e-02,  5.8461e-01, -1.9506e-02, -6.4998e-01, -4.9129e-01,\n",
      "          4.6330e-02,  2.3193e-01, -4.6691e-01, -3.2682e-01, -7.2814e-01,\n",
      "          9.0095e-02, -2.8878e-01,  6.8598e-02, -9.5864e-01,  2.1744e+00,\n",
      "          8.3597e-01, -3.7693e-01, -4.6789e-01,  5.1272e-01, -2.0898e-01,\n",
      "         -1.4173e+00,  3.0621e-01, -1.1107e+00, -4.6540e-01, -4.1512e-01,\n",
      "          4.8972e-01, -7.8386e-02, -6.3822e-01,  1.1082e-01, -4.8987e-01,\n",
      "         -1.3527e-01,  9.2404e-01, -6.8526e-01,  3.6914e+00, -1.2588e-01,\n",
      "         -3.5469e-01, -9.4230e-02, -6.8178e-01,  3.8612e-01, -2.1969e-01,\n",
      "         -1.3716e-01, -1.1294e+00, -6.6529e-01, -1.0089e+00,  2.8202e-01,\n",
      "         -1.6484e-01,  1.9298e-01, -5.3883e-01, -7.3170e-01,  3.8052e-01,\n",
      "         -3.2998e-01, -4.0463e-01, -5.6316e-01, -1.2669e+00, -6.4250e-03,\n",
      "         -4.8913e-01, -2.9134e-01, -5.0545e-01, -3.5455e-01, -4.2716e-01,\n",
      "          1.5791e-01, -6.8709e-01, -1.4152e+00, -8.2529e-01,  7.5635e-01,\n",
      "         -4.8494e-01, -3.0527e-01, -1.1189e+00, -2.6148e-01, -1.0113e-01,\n",
      "         -6.2588e-01, -5.1551e-01, -1.0975e+00, -5.3347e-01, -9.8451e-01,\n",
      "          6.3059e-01,  1.6396e-01, -1.0358e+00, -2.4855e-01, -4.1036e-01,\n",
      "         -4.9192e-02,  5.2686e-02, -4.0643e-01,  2.3632e+00,  2.2487e-01,\n",
      "          7.1494e-01,  3.6081e-01,  1.4085e+00,  1.7681e+00, -1.3796e+00,\n",
      "          2.2918e+00, -1.0672e+00,  4.3908e-01,  2.0997e+00, -9.6864e-01,\n",
      "          2.0238e+00,  2.4523e+00,  2.5393e+00, -8.5936e-01,  9.5342e-01,\n",
      "         -7.7026e-01, -2.4568e-01, -8.4988e-01, -6.7122e-01, -4.6205e-01,\n",
      "         -1.2140e+00,  2.8009e-01, -6.8000e-01, -1.0022e+00, -6.1658e-01,\n",
      "         -9.4509e-01, -2.5603e-01, -6.5955e-01, -9.7045e-02,  1.0932e+00,\n",
      "         -5.3778e-01, -9.3367e-01,  7.4512e-01, -7.2628e-01,  4.8979e-02,\n",
      "         -6.7683e-01, -1.4752e-01, -9.1286e-01, -5.8288e-01, -1.1370e+00,\n",
      "          4.3109e+00,  6.4330e-01, -1.3576e+00, -6.4657e-01,  9.1276e-01,\n",
      "         -2.8392e-01,  7.8516e-01,  7.8611e+00,  5.8800e+00,  3.4984e+00,\n",
      "         -3.6676e-01,  6.7279e-01,  8.4678e-03,  9.4247e-01,  1.2394e-01,\n",
      "          3.3725e-01, -6.5140e-02, -2.2568e-01, -4.7315e-01, -1.2867e+00,\n",
      "          3.4558e-01, -3.0102e-01, -3.3339e-01,  3.6029e-01, -8.4302e-01,\n",
      "         -2.5028e-01,  1.6634e-01, -1.3761e+00,  2.3350e-01, -1.3407e-01,\n",
      "          1.0478e-01,  7.3512e-02, -3.8161e-01, -2.4507e-01, -4.1723e-01,\n",
      "         -4.3910e-01,  4.0000e-01, -3.9092e-02, -4.9160e-01,  4.1487e-01]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "[52.79000473022461, 10.11531925201416, 7.28095006942749, 4.450318336486816, 1.5160844326019287]\n"
     ]
    }
   ],
   "source": [
    "print(top5_probabilities)\n",
    "print(top5_class_indices)\n",
    "print(output)\n",
    "print(top5_probabilities[0].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
